{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -q accelerate bitsandbytes safetensors langchain  datasets sentence-transformers chromadb youtube-transcript-api\n",
        "! pip install -q  git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "tHoyzbiKNLu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import  pipeline\n",
        "import torch\n",
        "import transformers\n",
        "from torch import cuda"
      ],
      "metadata": {
        "id": "L75PAZFZNVpA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bn22/Mistral-7B-Instruct-v0.1-sharded\""
      ],
      "metadata": {
        "id": "cyVgHyVWOEWq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto' ,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.bos_token_id = 0\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=2048,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "NwheIt2oOG0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ],
      "metadata": {
        "id": "EzA3zY_7ON_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader(\"0phRae42lqY\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "5MXUlIZ_ObrB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents,\n",
        "    embedding=embed_model,\n",
        "    persist_directory=\"DB\"\n",
        ")"
      ],
      "metadata": {
        "id": "y7fOrwb0OiBm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "rYHcB6SpPdT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    chain_type=\"stuff\" ,\n",
        "    llm=local_llm,\n",
        "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 1}),\n",
        "    return_source_documents=False,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "8oufmlDZPlUQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what is the problem we want to solve ?\t'\n",
        "\n",
        "vectordb.similarity_search(\n",
        "    query,  # the search query\n",
        "    k=1  # returns top 3 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "id": "wncctZdgR2HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_chain.run(\"what is the problem we want to solve ?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "V55wx3b0Pu3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}